{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc03a542-1c5a-4513-b810-0494db35caea",
   "metadata": {},
   "source": [
    "#### **Artificial Intelligence (AI)**\n",
    "Definition: AI is a broad field that focuses on creating systems capable of performing tasks that typically require human intelligence, such as understanding natural language, recognizing patterns, solving problems, and making decisions.\n",
    "#### **Data Science**\n",
    "Definition: Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data.\n",
    "\n",
    "AI and data science are interdependent fields. \n",
    "AI leverages data science techniques to learn from data and make intelligent decisions, while data science provides the methodologies and tools for analyzing and interpreting data, which are crucial for developing AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b6443-f65a-4763-9a7b-24fb1ef9774b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2525624e-2044-4207-86ae-144ccfd75800",
   "metadata": {},
   "source": [
    "**Artificial Intelligence (AI)** :\n",
    "- AI is the broadest category, referring to systems designed to perform tasks that typically require human intelligence.\n",
    "- This includes reasoning, learning, problem-solving, perception, and language understanding.\n",
    "- AI encompasses various technologies and methodologies, including ML and DL\n",
    "\n",
    "**Machine Learning (ML)**:\n",
    "- ML is a subset of artificial intelligence that enables systems to learn from data and improve their performance over time as they are exposed to more data without being explicitly programmed.\n",
    "- It involves algorithms that can identify patterns and make decisions based on data.\n",
    "\n",
    "**Deep Learning (DL)**:\n",
    "- DL is a further specialization within ML that employs artificial neural networks with multiple layers to analyze complex patterns in large datasets.\n",
    "- DL models require substantial computational resources and data to train effectively.\n",
    "\n",
    "**Natural Language Processing (NLP)**:\n",
    "- NLP is a field of AI focused on enabling computers to to understand, interpret, and generate human like language.\n",
    "- Combines ML/DL techniques with linguistics to analyze or generate text and speech.\n",
    "\n",
    "**GenAI** :\n",
    "- GenAI refers to a class of artificial intelligence models and it is a specific branch of DL that can generate new content, such as text, images, music, or other media, based on the patterns learned from existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b194a-0c0c-42b0-816b-74cd99f37ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef724b6-f0fc-4370-8c9a-846718a7f603",
   "metadata": {},
   "source": [
    "In the sentence:  \n",
    "**\"NLP combines ML/DL techniques with linguistics to analyze or generate text and speech.\"**  \n",
    "\n",
    "### **Linguistics** refers to the scientific study of language, including its structure, meaning, and usage. In NLP (Natural Language Processing), linguistics helps machines understand and process human language more effectively.  \n",
    "\n",
    "### **Key Areas of Linguistics in NLP:**  \n",
    "1. **Phonetics & Phonology** – Deals with speech sounds (useful in speech recognition).  \n",
    "2. **Morphology** – Studies word formation (e.g., \"running\" = \"run\" + \"-ing\").  \n",
    "3. **Syntax** – Examines sentence structure (e.g., grammar rules).  \n",
    "4. **Semantics** – Focuses on meaning (e.g., \"bank\" as a financial institution vs. riverbank).  \n",
    "5. **Pragmatics** – Understands context and implied meanings (e.g., sarcasm detection).  \n",
    "\n",
    "### **Why Combine Linguistics with ML/DL?**  \n",
    "- Machine learning and deep learning help NLP **learn from data**, while linguistics **provides rules and structure** for better accuracy.  \n",
    "- For example, **chatbots, translation models, and sentiment analysis** use both ML/DL and linguistic principles to function effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf992b-4428-4699-b679-f00e9472920d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc79c87e-40ec-4ddf-acd3-9ad6f6220507",
   "metadata": {},
   "source": [
    "\"Without being explicitly programmed\" in machine learning (ML) means that the system is not given specific, rule-based instructions for every possible scenario. Instead, it **learns patterns** from data and makes predictions or decisions based on that learning.  \n",
    "\n",
    "### Example:  \n",
    "#### **Traditional Programming (Explicitly Programmed)**\n",
    "- If you write a program to identify spam emails, you might use predefined rules like:\n",
    "  - If the subject contains \"Win a prize\" → Mark as spam  \n",
    "  - If the sender is unknown → Mark as spam  \n",
    "\n",
    "#### **Machine Learning (Not Explicitly Programmed)**\n",
    "- Instead of manually defining rules, an ML model is trained on a dataset of emails labeled as \"spam\" or \"not spam.\"  \n",
    "- The model **learns** from patterns in the data (e.g., word frequency, sender behavior) and generalizes to detect spam in new emails **without explicit rules written by a programmer**.  \n",
    "\n",
    "In short, ML models **learn and improve** from data instead of relying on manually coded instructions for every situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29bd05-90b2-49a1-a1cc-72ae99d9a73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c39f5f-46d9-4897-9d47-3b00e77d7af1",
   "metadata": {},
   "source": [
    "Machine learning can be broadly classified into several types based on how the algorithms learn and interact with data. Here are the main types:\n",
    "\n",
    "### **1. Supervised Learning**\n",
    "\n",
    "- **Description**: In supervised learning, the model is trained on a labeled dataset, meaning that each training example is paired with an output label.\n",
    "- **Examples**:\n",
    "  - **Classification**: Assigning labels to new instances (e.g., spam email detection).\n",
    "  - **Regression**: Predicting continuous values (e.g., house price prediction).\n",
    "\n",
    "### **2. Unsupervised Learning**\n",
    "\n",
    "- **Description**: In unsupervised learning, the model is given data without explicit labels and must find structure within the data.\n",
    "- **Goal**: Identify patterns, groupings, or features within the data.\n",
    "- **Examples**:\n",
    "  - **Clustering**: Grouping similar data points together (e.g., customer segmentation).\n",
    "  - **Dimensionality Reduction**: Reducing the number of features while preserving essential information (e.g., PCA).\n",
    "\n",
    "### **3. Semi-Supervised Learning**\n",
    "\n",
    "- **Description**: This approach lies between supervised and unsupervised learning. The model is trained on a dataset that contains a small amount of labeled data and a large amount of unlabeled data.\n",
    "- **Goal**: Improve learning performance by leveraging both labeled and unlabeled data.\n",
    "- **Examples**:\n",
    "  - Improving image recognition models with limited labeled images but many unlabeled ones.\n",
    "\n",
    "### **4. Reinforcement Learning**\n",
    "\n",
    "- **Description**: It learns through trial and error.\n",
    "   - A model learns to make decisions by interacting with an environment or agent and receiving feedback in the form of rewards or penalties.\n",
    "- **Goal**: Develop a policy that maximizes the expected cumulative reward over time.\n",
    "- **Examples**:\n",
    "  - Training autonomous vehicles to navigate.\n",
    "  - Developing AI for games like chess or Go.\n",
    "\n",
    "### **5. Self-Supervised Learning**\n",
    "\n",
    "- **Description**: A form of unsupervised learning where the model generates labels from the data itself. The model learns to predict part of the input data from other parts of the input data.\n",
    "- **Goal**: Create useful representations from unlabeled data without manual labeling.\n",
    "- **Examples**:\n",
    "  - Language models predicting the next word in a sentence (e.g., GPT-3).\n",
    "\n",
    "### **Choosing the Right Type**\n",
    "- Use **Supervised Learning** if you have a large, labeled dataset.\n",
    "- Use **Unsupervised Learning** if you need to explore or group data without labels.\n",
    "- Use **Semi-Supervised Learning** when labeled data is scarce but unlabeled data is abundant.\n",
    "- Use **Reinforcement Learning** for sequential decision-making problems where learning from interactions is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f520bf7-3f58-452d-a7e3-09dcc007655e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3abb58-4cc2-496d-bc75-17a8ceb2b957",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- It is predictive model used to find linear relationship between dependent variable (target) and one or more independent variables\n",
    "(features) \n",
    "- linear : straight line or path.\n",
    "- The primary goal is to predict the target variable based on the input features by fitting a linear equation to the observed data.\n",
    "regression : prediction of conteneous values or real number\n",
    "\n",
    "Target column : conteneous values\n",
    "   prices, sales, age, weight, temp, salary etc\n",
    "\n",
    "it is parametric algorithm :  assumption on data distribution. In the case of linear regression, the assumed form is a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e104ef-59b9-4a43-b3d5-0741466cd02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bd1bc60-c3a6-498a-883f-21e74a7437df",
   "metadata": {},
   "source": [
    "#### Best Fit line\n",
    "1. Lowest / least Mean squared error\n",
    "2. it passes through maximum number of data points\n",
    "3. best m & c values\n",
    "4. Gradient descent algorithm find one best fit line from infinite number of possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d631702-379d-4f35-8de3-4260270b587e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f77d4160-02f8-46f3-86d6-e33281fb651a",
   "metadata": {},
   "source": [
    "The coefficient of correlation, often denoted as \\( r \\), is a statistical measure that quantifies the strength and direction of the relationship between two variables. It ranges from -1 to 1, where:\n",
    "\n",
    "- **\\( r = 1 \\)**: Perfect positive correlation (as one variable increases, the other also increases).\n",
    "- **\\( r = -1 \\)**: Perfect negative correlation (as one variable increases, the other decreases).\n",
    "- **\\( r = 0 \\)**: No correlation (no linear relationship between the variables).\n",
    "\n",
    "### **Types of Correlation Coefficients**\n",
    "\n",
    "1. **Pearson Correlation Coefficient**:\n",
    "   - **Definition**: Measures the linear relationship between two continuous variables.\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n \\sum x^2 - (\\sum x)^2][n \\sum y^2 - (\\sum y)^2]}}\n",
    "     \\]\n",
    "   - **Interpretation**: Values close to 1 or -1 indicate strong correlation, while values close to 0 indicate weak or no correlation.\n",
    "\n",
    "\n",
    "### **Example: Pearson Correlation in Python**\n",
    "\n",
    "Here's how you can calculate the Pearson correlation coefficient using Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "correlation, _ = pearsonr(x, y)\n",
    "print(\"Pearson Correlation Coefficient:\", correlation)\n",
    "```\n",
    "\n",
    "In this example, the Pearson correlation coefficient would be 1, indicating a perfect positive linear relationship between `x` and `y`.\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- **Strong Positive Correlation (\\(0.7 \\leq r \\leq 1\\))**: As one variable increases, the other variable tends to increase.\n",
    "- **Strong Negative Correlation (\\(-1 \\leq r \\leq -0.7\\))**: As one variable increases, the other variable tends to decrease.\n",
    "- **Moderate Correlation (\\(0.3 \\leq |r| < 0.7\\))**: There is a noticeable, but not perfect, linear relationship.\n",
    "- **Weak Correlation (\\(0 \\leq |r| < 0.3\\))**: There is a very weak or no linear relationship.\n",
    "\n",
    "The coefficient of correlation is a powerful tool for understanding the linear relationship between two variables and is widely used in statistics and data science.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary Table**  \n",
    "| Correlation Coefficient (r) | Strength | Relationship |\n",
    "|-----------------------------|----------|--------------|\n",
    "| **0.7 to 1** | Strong Positive | As X ↑, Y ↑ significantly |\n",
    "| **-1 to -0.7** | Strong Negative | As X ↑, Y ↓ significantly |\n",
    "| **0.3 to 0.7** or **-0.3 to -0.7** | Moderate | Noticeable trend but not perfect |\n",
    "| **0 to 0.3** or **0 to -0.3** | Weak | Very little or no clear trend |\n",
    "\n",
    "Let me know if you need any clarifications! 🚀\n",
    "r=covariance/std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f180a96-68bc-49f7-8476-f3a7b520aec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbb58763-b012-49b0-87d2-766e17e19d79",
   "metadata": {},
   "source": [
    "**No, cost function and loss function are not exactly the same, but they are closely related.**  \n",
    "\n",
    "### 🔹 **Loss Function**  \n",
    "- It calculates the error for a **single data point** (i.e., one training example).  \n",
    "- Example:  \n",
    "  - Mean Squared Error (MSE) for regression:  \n",
    "    \\[\n",
    "    L(y, \\hat{y}) = (\\hat{y} - y)^2\n",
    "    \\]\n",
    "  - Cross-Entropy Loss for classification:  \n",
    "    \\[\n",
    "    L(y, \\hat{y}) = - y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n",
    "    \\]\n",
    "\n",
    "### 🔹 **Cost Function**  \n",
    "- It is the **average** of the loss function over the entire dataset.  \n",
    "- Example:  \n",
    "  - Mean Squared Error (MSE) as cost function:  \n",
    "    \\[\n",
    "    J(w) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
    "    \\]\n",
    "\n",
    "### **Key Difference**  \n",
    "✔ **Loss function → Single example error**  \n",
    "✔ **Cost function → Average error over all examples**  \n",
    "\n",
    "In short, the cost function is a generalized form of the loss function across the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9572b7d-a38e-41e4-b860-ed82b97ad908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed38156f-3ba8-4891-8fa1-2e78fde6dade",
   "metadata": {},
   "source": [
    "### **Gradient Descent Algorithm – Explained Simply**  \n",
    "\n",
    "Gradient Descent is an **iterative optimization algorithm** used in machine learning to **minimize a cost function** (a cost/loss function) by iteratively adjusting its parameters. It helps find the **best model weights** that reduce errors in predictions.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**  \n",
    "1. **Start with Initial Parameters (Random Weights & Bias)**  \n",
    "   - At the beginning, the algorithm starts with random values for the model parameters (e.g., weights in linear regression or neural networks).  \n",
    "\n",
    "2. **Compute the Cost Function**  \n",
    "   - The cost function (like Mean Squared Error in regression) measures how far the model’s predictions are from the actual values.  \n",
    "   - The goal is to **minimize this function** by adjusting the parameters.  \n",
    "\n",
    "3. **Compute the Gradient (Slope of the Cost Function)**  \n",
    "   - The gradient (derivative of the cost function) tells us the **direction** in which the parameters should be updated.  \n",
    "   - If the gradient is positive, we move **left (decrease the parameter)**.  \n",
    "   - If the gradient is negative, we move **right (increase the parameter)**.  \n",
    "\n",
    "4. **Update Parameters Using the Learning Rate (Step Size)**  \n",
    "   - New parameter value = Old parameter value - (Learning Rate × Gradient)  \n",
    "   - The **learning rate (α)** controls how big each step is:  \n",
    "     - Too **high** → May **overshoot** the minimum.  \n",
    "     - Too **low** → Converges **slowly** or gets stuck.  \n",
    "\n",
    "5. **Repeat Until Convergence**  \n",
    "   - The process repeats until the cost function **stops decreasing** (i.e., reaches the lowest point or local minimum).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Gradient Descent**  \n",
    "1. **Batch Gradient Descent**  \n",
    "   - Uses the **entire dataset** to compute the gradient in each step.  \n",
    "   - **Pros**: More accurate updates.  \n",
    "   - **Cons**: Slow for large datasets.  \n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**  \n",
    "   - Updates parameters after **each data point** (instead of the whole dataset).  \n",
    "   - **Pros**: Faster, works well for large datasets.  \n",
    "   - **Cons**: More noise (fluctuations in updates).  \n",
    "\n",
    "3. **Mini-Batch Gradient Descent**  \n",
    "   - Uses a **small batch of data** (instead of one point or the entire dataset).  \n",
    "   - **Pros**: Balances accuracy and speed.  \n",
    "   - **Most commonly used** in deep learning.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Gradient Descent in Linear Regression**  \n",
    "Let's say we want to predict **house prices** based on square footage.  \n",
    "- **Step 1**: Start with random weights (w) and bias (b).  \n",
    "- **Step 2**: Compute the cost function (Mean Squared Error).  \n",
    "- **Step 3**: Calculate the gradient (derivative of cost function).  \n",
    "- **Step 4**: Update weights:  \n",
    "  \\[\n",
    "  w = w - \\alpha \\times \\frac{d}{dw} J(w)\n",
    "  \\]\n",
    "  \\[\n",
    "  b = b - \\alpha \\times \\frac{d}{db} J(b)\n",
    "  \\]\n",
    "- **Step 5**: Repeat until the cost function is minimized.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization**  \n",
    "Imagine standing on a mountain and trying to reach the lowest valley (global minimum).  \n",
    "- If you **take small steps**, it takes longer but ensures you reach the bottom.  \n",
    "- If you **take large steps**, you might **overshoot** or miss the valley.  \n",
    "- The gradient tells you **which direction to go**, and the learning rate controls **step size**.  \n",
    "\n",
    "Would you like me to explain this with Python code? 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68668bf-0086-4a0d-b075-26f89f9dab89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee76c3-3d19-4eb3-a180-0ea2eda66f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea478a1-09f1-4699-b632-397b02763bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24db063-1bae-403e-8268-d9d3dd38681f",
   "metadata": {},
   "source": [
    "When we say that the variation in the dependent variable is \"explained by the independent variables,\" we're referring to the extent to which the independent variables account for changes in the dependent variable. In other words, it's about how well the independent variables can predict the dependent variable.\n",
    "\n",
    "### **Breakdown**\n",
    "\n",
    "1. **Dependent Variable (Target)**:\n",
    "   - This is the outcome or the variable you want to predict or explain.\n",
    "   - Example: House price.\n",
    "\n",
    "2. **Independent Variables (Predictors)**:\n",
    "   - These are the variables that you use to predict or explain the dependent variable.\n",
    "   - Examples: Square footage, number of bedrooms, age of the house.\n",
    "\n",
    "### **Explanation with Regression**\n",
    "\n",
    "In regression analysis, we use the independent variables to model and predict the dependent variable. The \"explained variation\" refers to how much of the total variability in the dependent variable can be attributed to the independent variables.\n",
    "\n",
    "#### **Total Variation (SST)**\n",
    "- **SST (Total Sum of Squares)**: Measures the total variation in the dependent variable (e.g., the total spread of house prices from the mean house price).\n",
    "\n",
    "#### **Explained Variation (SSR)**\n",
    "- **SSR (Regression Sum of Squares)**: Measures the variation that is explained by the regression model. It represents the part of the total variation in the dependent variable that can be attributed to the relationship with the independent variables.\n",
    "  - In essence, SSR quantifies how much of the change in the house prices can be accounted for by changes in square footage, number of bedrooms, etc.\n",
    "\n",
    "#### **Unexplained Variation (SSE)**\n",
    "- **SSE (Error Sum of Squares)**: Measures the variation that is not explained by the regression model. It represents the part of the total variation in the dependent variable that is due to factors other than the independent variables (e.g., random noise, unmeasured variables).\n",
    "\n",
    "### **R-squared (\\( R^2 \\))**\n",
    "\n",
    "- **Definition**: \\( R^2 \\) is a statistical measure that represents the proportion of the total variation in the dependent variable that is explained by the independent variables.\n",
    "-  - R² (Coefficient of Determination) measures how well a regression model explains the variaance of the dependent variable by the independent variables.\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "  \\]\n",
    "- **Interpretation**:\n",
    "  - An \\( R^2 \\) value close to 1 indicates that a large proportion of the variation in the dependent variable is explained by the independent variables, implying a good fit of the model.\n",
    "  - An \\( R^2 \\) value close to 0 indicates that the independent variables do not explain much of the variation in the dependent variable, implying a poor fit of the model.\n",
    "\n",
    "### **Example**\n",
    "\n",
    "Let's say you're predicting house prices based on square footage and number of bedrooms:\n",
    "\n",
    "- **High \\( R^2 \\)**: If square footage and number of bedrooms can predict house prices very well, most of the variation in house prices is explained by these two variables, resulting in a high \\( R^2 \\) value.\n",
    "- **Low \\( R^2 \\)**: If other factors (like location, market conditions) not included in the model have a significant impact on house prices, then square footage and number of bedrooms alone might not explain the variation well, resulting in a low \\( R^2 \\) value.\n",
    "\n",
    "In summary, \"explained by the independent variables\" means that we are quantifying how much of the variability in the dependent variable can be attributed to the independent variables, helping us understand the effectiveness of our model.\n",
    "\n",
    "Feel free to ask if you have more questions or need further clarification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8555d4-b46e-46a9-878e-58b08a05ab58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eeeafee-f074-462d-9800-cc5b08cc4d4b",
   "metadata": {},
   "source": [
    "Yes, **R² (coefficient of determination) can be negative** — but only in certain situations.\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 **When R² is Negative**\n",
    "\n",
    "A negative R² means:\n",
    "\n",
    "> The model fits **worse than a horizontal line at the mean of the target** — in other words, it's **worse than doing nothing**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Why Can This Happen?\n",
    "\n",
    "- R² is calculated as:\n",
    "\n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\text{SS}_{\\text{res}} \\) = Sum of squares of residuals (errors from model)\n",
    "- \\( \\text{SS}_{\\text{tot}} \\) = Total sum of squares (errors from mean model)\n",
    "\n",
    "- If your model is **very bad**, \\( \\text{SS}_{\\text{res}} > \\text{SS}_{\\text{tot}} \\), which makes \\( R^2 < 0 \\)\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ When You Might See This:\n",
    "- Model is poorly trained\n",
    "- Wrong or irrelevant features\n",
    "- Overfitting or underfitting\n",
    "- Predicting on data far outside the training range\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Key Takeaway:\n",
    "- **R² ranges from -∞ to 1** in general\n",
    "- **Negative R² = worse than predicting the mean**\n",
    "- **R² = 0** → model predicts no better than the mean\n",
    "- **R² = 1** → perfect prediction\n",
    "\n",
    "Would you like a small example where a model gives negative R²?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194e370-43ff-4663-b9dc-b63b1095e04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65ea310c-f25f-46aa-bc8e-4250b1831f9b",
   "metadata": {},
   "source": [
    "The **adjusted \\( R^2 \\)** score is a modified version of the \\( R^2 \\) score that takes into account the number of predictors in the model. Unlike the \\( R^2 \\) score, which always increases when you add more variables, the adjusted \\( R^2 \\) penalizes the addition of unnecessary predictors. This makes it a more reliable metric for model evaluation, especially when comparing models with different numbers of predictors.\n",
    "\n",
    "### **Adjusted \\( R^2 \\) Formula**\n",
    "\n",
    "The adjusted \\( R^2 \\) is calculated using the following formula:\n",
    "\\[ \n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n",
    "\\]\n",
    "where:\n",
    "- \\( R^2 \\) = Coefficient of determination (standard \\( R^2 \\) score)\n",
    "- \\( n \\) = Number of observations\n",
    "- \\( p \\) = Number of predictors (independent variables)\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- **Higher Adjusted \\( R^2 \\)**: Indicates a better fit of the model to the data, while considering the number of predictors used.\n",
    "- **Lower Adjusted \\( R^2 \\)**: Suggests that either the model does not fit the data well or that adding more predictors does not significantly improve the fit.\n",
    "\n",
    "### **Example Calculation**\n",
    "\n",
    "Suppose you have a regression model with the following values:\n",
    "- \\( R^2 = 0.85 \\)\n",
    "- \\( n = 100 \\) (number of observations)\n",
    "- \\( p = 5 \\) (number of predictors)\n",
    "\n",
    "Plugging these values into the formula:\n",
    "\\[\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - 0.85)(100 - 1)}{100 - 5 - 1} \\right) = 1 - \\left( \\frac{(0.15 \\times 99)}{94} \\right) = 1 - \\left( \\frac{14.85}{94} \\right) = 1 - 0.158 = 0.842\n",
    "\\]\n",
    "\n",
    "### **When to Use Adjusted \\( R^2 \\)**\n",
    "\n",
    "- **Model Comparison**: When comparing multiple regression models with a different number of predictors.\n",
    "- **Model Selection**: To determine the optimal number of predictors that provide a good fit without overfitting.\n",
    "- **Model Evaluation**: To assess the overall explanatory power of the regression model while accounting for the complexity of the model.\n",
    "\n",
    "### **Advantages of Adjusted \\( R^2 \\)**\n",
    "\n",
    "- **Penalty for Complexity**: Penalizes the addition of irrelevant predictors, thus discouraging overfitting.\n",
    "- **Balanced Metric**: Provides a more balanced measure of model performance compared to the standard \\( R^2 \\).\n",
    "\n",
    "In summary, the adjusted \\( R^2 \\) score is a valuable metric for evaluating the performance of regression models, especially when dealing with multiple predictors. It helps ensure that the model is both accurate and parsimonious.\n",
    "\n",
    "Feel free to ask if you have any more questions or need further clarification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6cd89-6f09-449a-8774-2784e37c79d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b2e3c6-da82-405a-9918-28bed29da153",
   "metadata": {},
   "source": [
    "When performing regression analysis, it's crucial to ensure that certain assumptions are met for the model to be valid and reliable. Here are the key assumptions in regression analysis:\n",
    "\n",
    "### **1. Linearity**\n",
    "- **Definition**: The relationship between the independent variables and the dependent variable is linear.\n",
    "- **Implication**: The change in the dependent variable is proportional to the change in the independent variables.\n",
    "- **Check**: Scatter plots and residual plots can help visualize linearity.\n",
    "\n",
    "### **2. Independence  (No Autocorrelation)**\n",
    "- **Definition**: The residuals (errors) are independent of each other.\n",
    "- **Implication**: There should be no correlation between consecutive residuals.\n",
    "- **Check**: Durbin-Watson statistic can be used to detect autocorrelation.\n",
    "\n",
    "### **3. Homoscedasticity (Constant Variance of Errors)**\n",
    "- **Definition**: The variance of residuals is constant across all levels of the independent variables.\n",
    "- **Implication**: The spread of residuals should be the same across the range of predicted values.\n",
    "- **Check**: Residual plots can help assess homoscedasticity.\n",
    "\n",
    "### **4. Normality**\n",
    "- **Definition**: The residuals of the model are normally distributed.\n",
    "- **Implication**: The residuals should form a normal distribution when plotted.\n",
    "- **Check**: Histogram or Q-Q plot of residuals can help check normality.\n",
    "\n",
    "### **5. No Multicollinearity  (Independent Variables Should Not Be Highly Correlated)**\n",
    "- **Definition**: Independent variables are not highly correlated with each other.\n",
    "- **Implication**: High multicollinearity can distort the estimated coefficients and make them unreliable.\n",
    "- **Check**: Variance Inflation Factor (VIF) can be used to detect multicollinearity.\n",
    "\n",
    "### **Why These Assumptions Matter**\n",
    "\n",
    "- **Linearity**: Ensures the model provides an unbiased estimate of the relationship.\n",
    "- **Independence**: Prevents overfitting and ensures the model generalizes well to new data.\n",
    "- **Homoscedasticity**: Ensures consistent variance in residuals, leading to reliable estimates.\n",
    "- **Normality**: Ensures valid hypothesis tests for coefficients.\n",
    "- **No Multicollinearity**: Ensures reliable and interpretable coefficient estimates.\n",
    "\n",
    "### **Example of Checking Assumptions**\n",
    "\n",
    "Here’s a brief example using Python to check these assumptions:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import shapiro, probplot\n",
    "\n",
    "# Assume df is your DataFrame containing the data\n",
    "X = df[['independent_var1', 'independent_var2']]  # Replace with your independent variables\n",
    "y = df['dependent_var']  # Replace with your dependent variable\n",
    "\n",
    "# Add a constant to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Residuals\n",
    "residuals = model.resid\n",
    "\n",
    "# Linearity\n",
    "plt.scatter(model.fittedvalues, residuals)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual plot')\n",
    "plt.show()\n",
    "\n",
    "# Homoscedasticity\n",
    "plt.scatter(model.fittedvalues, residuals)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Homoscedasticity check')\n",
    "plt.show()\n",
    "\n",
    "# Normality\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.title('Residuals Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot\n",
    "probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()\n",
    "\n",
    "# Independence (Durbin-Watson)\n",
    "dw_statistic = sm.stats.stattools.durbin_watson(residuals)\n",
    "print(f'Durbin-Watson statistic: {dw_statistic}')\n",
    "\n",
    "# Multicollinearity (VIF)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data)\n",
    "```\n",
    "\n",
    "By ensuring these assumptions are met, you can increase the validity and reliability of your regression model.\n",
    "\n",
    "Feel free to ask if you have any more questions or need further clarification!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeeac28-623f-45f1-964e-a3b8da523b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6faac5-a8ef-4d10-9034-b3b28d599ab0",
   "metadata": {},
   "source": [
    "### **Variance Inflation Factor (VIF)**\n",
    "\n",
    "Variance Inflation Factor (VIF) is a measure used to detect multicollinearity in a regression analysis. Multicollinearity occurs when independent variables in a regression model are highly correlated with each other. High multicollinearity can distort the estimation of the coefficients and make them unreliable.  If VIF > 10, remove or combine highly correlated features to avoid overfitting.\n",
    "\n",
    "2. **Formula**:\n",
    "   - VIF for a predictor \\( X_i \\) is calculated as:\n",
    "     \\[\n",
    "     \\text{VIF}(X_i) = \\frac{1}{1 - R^2_i}\n",
    "     \\]\n",
    "   - Here, \\( R^2_i \\) is the coefficient of determination obtained by regressing \\( X_i \\) on all other predictors in the model.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - **VIF = 1**: No correlation between the predictor and other predictors.\n",
    "   - **1 < VIF < 5**: Moderate correlation, usually acceptable.\n",
    "   - **VIF ≥ 5**: Indicates a high correlation, suggesting multicollinearity.\n",
    "\n",
    "### **Why VIF Is Important**\n",
    "\n",
    "- **Detecting Multicollinearity**: High VIF values indicate that the predictor variables are highly correlated, which can make the regression coefficients unstable and difficult to interpret.\n",
    "- **Improving Model Accuracy**: Identifying and addressing multicollinearity can improve the accuracy and reliability of the regression model.\n",
    "\n",
    "### **Example Calculation in Python**\n",
    "\n",
    "Here's how you can calculate VIF using Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'X1': [10, 20, 30, 40, 50],\n",
    "    'X2': [5, 10, 15, 20, 25],\n",
    "    'X3': [2, 4, 6, 8, 10],\n",
    "    'Y': [1, 2, 3, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variables\n",
    "X = df[['X1', 'X2', 'X3']]\n",
    "\n",
    "# Add a constant to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)\n",
    "```\n",
    "\n",
    "### **Output Example**\n",
    "\n",
    "```plaintext\n",
    "  Variable       VIF\n",
    "0      const  10.0000\n",
    "1        X1   1.0000\n",
    "2        X2   1.0000\n",
    "3        X3   1.0000\n",
    "```\n",
    "\n",
    "In this example, all the VIF values are 1, indicating no multicollinearity among the predictors.\n",
    "\n",
    "### **How to Handle Multicollinearity**\n",
    "\n",
    "If high VIF values are detected, you can take the following steps to address multicollinearity:\n",
    "1. **Remove Highly Correlated Predictors**: Eliminate one or more of the highly correlated variables.\n",
    "2. **Combine Predictors**: Combine correlated predictors into a single variable (e.g., using Principal Component Analysis).\n",
    "3. **Regularization**: Apply regularization techniques such as Ridge Regression or Lasso Regression that can handle multicollinearity.\n",
    "\n",
    "### **Theoretical Example: Multicollinearity in Regression Analysis**\n",
    "\n",
    "#### **Scenario**\n",
    "\n",
    "Imagine you're building a multiple linear regression model to predict a student's exam score (dependent variable) based on three independent variables:\n",
    "1. **Hours of Study (X1)**\n",
    "2. **Class Attendance (X2)**\n",
    "3. **Tutoring Hours (X3)**\n",
    "\n",
    "#### **Why Multicollinearity Matters**\n",
    "\n",
    "- **Suppose**: Hours of Study (X1) and Class Attendance (X2) are highly correlated. Students who study more also tend to attend more classes.\n",
    "- **Problem**: This high correlation (multicollinearity) can distort the estimation of regression coefficients, leading to unreliable and unstable predictions.\n",
    "\n",
    "### **Using VIF to Detect Multicollinearity**\n",
    "\n",
    "#### **Step-by-Step Process**\n",
    "\n",
    "1. **Fit a Regression Model**: Start by fitting a multiple linear regression model with all three independent variables.\n",
    "   \n",
    "2. **Calculate VIF for Each Predictor**:\n",
    "   - For **X1 (Hours of Study)**: Regress X1 on X2 and X3. Calculate the \\( R^2 \\) from this auxiliary regression and then compute the VIF.\n",
    "     \\[\n",
    "     \\text{VIF}(X1) = \\frac{1}{1 - R^2_{X1|(X2, X3)}}\n",
    "     \\]\n",
    "\n",
    "   - For **X2 (Class Attendance)**: Regress X2 on X1 and X3. Calculate the \\( R^2 \\) from this auxiliary regression and then compute the VIF.\n",
    "     \\[\n",
    "     \\text{VIF}(X2) = \\frac{1}{1 - R^2_{X2|(X1, X3)}}\n",
    "     \\]\n",
    "\n",
    "   - For **X3 (Tutoring Hours)**: Regress X3 on X1 and X2. Calculate the \\( R^2 \\) from this auxiliary regression and then compute the VIF.\n",
    "     \\[\n",
    "     \\text{VIF}(X3) = \\frac{1}{1 - R^2_{X3|(X1, X2)}}\n",
    "     \\]\n",
    "\n",
    "3. **Interpret the VIF Values**:\n",
    "   - Suppose the VIF for **X1** is 8. This indicates that the variance of the regression coefficient for Hours of Study is inflated by a factor of 8 due to its correlation with the other predictors (Class Attendance and Tutoring Hours).\n",
    "   - Similarly, if the VIF for **X2** is 7, it indicates high multicollinearity between Class Attendance and the other predictors.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "VIF is a valuable diagnostic tool that helps ensure the reliability and interpretability of multiple linear regression models by detecting multicollinearity. By addressing high VIF values, you can improve the stability and accuracy of your model's predictions.\n",
    "\n",
    "I hope this theoretical example clarifies why VIF is used and how it helps in regression analysis. If you have more questions or need further clarification, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0e839-01bc-4373-a304-ed3fe52501e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58057325-58f6-4789-a9e2-7268aef18f45",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "An outlier is extreme data point that significantly differs from the other observations in a dataset. Outliers can arise due to variability in the data, measurement errors, or experimental errors. They can have a considerable impact on statistical analyses and machine learning models, as they can skew the results and lead to misleading conclusions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894755c0-8652-4c9d-85d7-44985b84bfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c34ef488-716b-491e-823a-d387d9112e34",
   "metadata": {},
   "source": [
    "### **Bias**\n",
    "\n",
    "- **Definition**: Bias in machine learning refers to systematic errors in algorithms that can lead to prejudiced outcomes. It can arise from various sources, including biased training data, selection bias, and model assumptions, impacting the fairness and accuracy of AI systems.\n",
    "- **High Bias**: Models with high bias are usually too simple and fail to capture the underlying patterns in the data. This leads to **underfitting**.\n",
    "- **Low Bias**: Models with low bias can capture the underlying patterns more accurately, but they may also capture noise and random fluctuations along with them if they are too complex.\n",
    "\n",
    "**Example**:\n",
    "- **Underfit** - Suppose you use a linear model to predict house prices based on their square footage, but the relationship between square footage and price is actually quadratic. The linear model will have high bias and underfit the data, missing the true relationship.\n",
    "- **Overfit**: A very complex model, such as a high-degree polynomial, may perform well on the training data perfectly but fail to perform well on new data, leading to overfitting.\n",
    "\n",
    "### **Variance**\n",
    "\n",
    "- **Definition**: It represents how much the model's predictions would change if it were trained on a different dataset.\n",
    "-  High variance indicates that the model is too complex, overfitting the training data by capturing noise and idiosyncrasies rather than underlying patterns.\n",
    "-  This results in excellent performance on training data but poor generalization to unseen data\n",
    "- **High Variance**: Models with high variance are too complex and capture noise and random fluctuations in the training data. This leads to overfitting.\n",
    "- **Low Variance**: Models with low variance are more stable and generalize better to new data.\n",
    "- Such models may perform poorly on both training and test datasets leads to underfit.\n",
    "\n",
    "\n",
    "### **Overfitting**\n",
    "\n",
    "Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data but also the noise and random fluctuations. This results in a model that performs exceptionally well on the training data but poorly on unseen or new data. Overfitting is a common issue in machine learning and can lead to misleading conclusions and poor generalization.\n",
    "\n",
    "### **Underfitting**\n",
    "\n",
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. This results in poor performance on both the training data and unseen data. Underfitting typically happens when the model has high bias and fails to represent the complexity of the data.\n",
    "\n",
    "### **Bias-Variance Tradeoff**\n",
    "\n",
    "The goal in machine learning is to find a balance between bias and variance to minimize the total error. This is known as the bias-variance tradeoff.\n",
    "\n",
    "- **High Bias, Low Variance**: Simple models with high bias may not capture the underlying patterns well (underfitting), but they are stable and have low variance.\n",
    "- **Low Bias, High Variance**: Complex models with low bias may capture the underlying patterns and noise (overfitting), leading to high variance.\n",
    "- **Optimal Balance**: A model with a good balance between bias and variance will capture the true patterns in the data and generalize well to new data.\n",
    "\n",
    "### **Visual Representation**\n",
    "\n",
    "Imagine a target with a bullseye:\n",
    "- **High Bias, Low Variance (Underfitting)**:\n",
    "  - The arrows are close together but far from the bullseye.\n",
    "- **Low Bias, High Variance (Overfitting)**:\n",
    "  - The arrows are spread out, hitting all over the target, including the bullseye.\n",
    "- **Low Bias, Low Variance (Good Fit)**:\n",
    "  - The arrows are close together and near the bullseye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13643a2-bcdf-498c-bc84-50407b7abce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d31bcac-7951-4ff0-846a-56a84219f45c",
   "metadata": {},
   "source": [
    "1) **scaling** :\n",
    "   - Normalization : (0 to 1) : minmaxscalar :if data is not normaly distributed then only we have to go for normalization\n",
    "   -  Standardization (-3 to +3) :Standarscalar : if data is normaly distributed then only we have to go for Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c33b70-508b-4786-8e63-8a9d560250ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15b65a50-b73d-411d-a67d-641a9b5723a8",
   "metadata": {},
   "source": [
    "### **Regularization**  \n",
    "\n",
    "- Regularization is a technique used to **prevent overfitting** by adding a penalty to the model’s complexity.\n",
    "- It discourages the model from learning excessive details or noise from the training data, helping it generalize better to unseen data.\n",
    "- Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages large model weights.\n",
    "- This results in simpler models (e.g., smoother or less flexible regression lines) that may have slightly lower training accuracy but better generalization to test data.\n",
    "- regularization’s job is to simplify the model — but underfitting happens when the model is already too simple.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Regularization Needed?**  \n",
    "When a model is too complex (e.g., too many features or too deep a neural network), it can:\n",
    "- Fit the training data **too well** (memorizing noise instead of learning patterns).\n",
    "- Perform poorly on **new/unseen data** (high variance problem).\n",
    "  \n",
    "Regularization **controls the complexity** of the model by penalizing large coefficients (weights) in linear models or imposing constraints in deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Regularization**\n",
    "1. **L1 Regularization (Lasso Regression)**\n",
    "   - Adds a **penalty proportional to the absolute value** of coefficients.\n",
    "   - Can shrink some coefficients to **zero**, effectively performing **feature selection**.\n",
    "   - Useful when you suspect that **only a few features are important**.\n",
    "   - This means: it automatically removes unimportant features 🔥\n",
    "\n",
    "   **Mathematical formula:**  \n",
    "   \\[\n",
    "   Loss = \\text{MSE} + \\lambda \\sum |w_i|\n",
    "   \\]\n",
    "\n",
    "2. **L2 Regularization (Ridge Regression)**\n",
    "   - Adds a **penalty proportional to the squared value** of coefficients.\n",
    "   - Shrinks coefficients **close to zero** but does not eliminate them.\n",
    "   - Helps in handling **multicollinearity** and improving stability.\n",
    "   - You have many features that are all slightly useful\n",
    "   - Keeps all features in the model but reduces their impact\n",
    "   - You want to reduce overfitting, but not drop features\n",
    "\n",
    "   **Mathematical formula:**  \n",
    "   \\[\n",
    "   Loss = \\text{MSE} + \\lambda \\sum w_i^2\n",
    "   \\]\n",
    "\n",
    "3. **Elastic Net Regularization**\n",
    "   - A combination of **L1 and L2** regularization.\n",
    "   - Useful when dealing with **many correlated features**.\n",
    "   - Adjusted using a mixing parameter (\\(\\alpha\\)).\n",
    "\n",
    "---\n",
    "\n",
    "### **Regularization in Python**\n",
    "Using **Ridge (L2) and Lasso (L1) regression**:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Ridge Regression (L2)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Apply Lasso Regression (L1)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "print(\"Lasso Coefficients:\", lasso.coef_)  # Some coefficients may be zero\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Which Regularization?**\n",
    "- **L1 (Lasso)** → When you need **feature selection** (sparse models). Used when less features like 10 .\n",
    "- **L2 (Ridge)** → When all features contribute but need **stabilization**.  Used when more features are there.\n",
    "- **Elastic Net** → When you have **correlated features** and need both effects.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f618dc2-8c03-48d1-8de2-e02513d6b441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aecdca14-5079-45f6-924b-3881e11cac8d",
   "metadata": {},
   "source": [
    "### **Cross-Validation in Machine Learning**  \n",
    "\n",
    "Cross-validation (CV) is a technique used to evaluate the performance of a machine learning model by splitting the dataset into multiple subsets.  \n",
    " Cross-validation ensures that every part of the dataset to be both **training and test sets**, making the evaluation **more reliable**.\n",
    "\n",
    "It helps:\n",
    "- Prevent **overfitting** (ensuring the model generalizes well to unseen data).\n",
    "- Reduce **bias and variance** in performance estimates.\n",
    "- Select the best **hyperparameters** for the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. K-Fold Cross-Validation (Most Common)**\n",
    "- Divides data into **K** equal parts (folds).\n",
    "- The model is trained on **K-1** folds and tested on the remaining fold.\n",
    "- The process is repeated **K** times, with each fold serving as the test set once.\n",
    "- The final score is the **average** of all K iterations.\n",
    "\n",
    "✅ **Advantage**: More stable than a single train-test split.  \n",
    "❌ **Disadvantage**: Computationally expensive for large datasets.\n",
    "\n",
    "**Example (5-Fold CV):**  \n",
    "1. Train on folds **1,2,3,4**, test on **5**  \n",
    "2. Train on folds **1,2,3,5**, test on **4**  \n",
    "3. Train on folds **1,2,4,5**, test on **3**  \n",
    "4. Train on folds **1,3,4,5**, test on **2**  \n",
    "5. Train on folds **2,3,4,5**, test on **1**  \n",
    "➡️ The final performance is the **average** of all 5 runs.\n",
    "\n",
    "### **Cross-Validation in Python**\n",
    "Using **K-Fold Cross-Validation** in `scikit-learn`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1)\n",
    "\n",
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Mean CV Score:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Cross-Validation?**\n",
    "- **Small datasets** → Prevents overfitting when data is limited.\n",
    "- **Hyperparameter tuning** → Used in **GridSearchCV** or **RandomizedSearchCV**.\n",
    "- **Imbalanced classification** → Use **StratifiedKFold** to balance classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eefd85-7543-4d20-b815-6d3fb8a810d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90dd22c1-dbf4-44ee-9881-f8ab782d44fb",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning in Linear Regression**  \n",
    "\n",
    "#### **What is Hyperparameter Tuning?**  \n",
    "Hyperparameter tuning is the process of selecting the best hyperparameters for a machine learning model to optimize its performance.\n",
    "Unlike others model parameters (e.g., weights in neural networks), **hyperparameters** are set **before** training begins and must be optimized manually or through automated search.\n",
    "\n",
    "In the context of linear regression, hyperparameter tuning often involves selecting the best regularization parameters to improve the model's performance and prevent overfitting.\n",
    "\n",
    "we use regularization techniques like Ridge (L2) and Lasso (L1) Regression, we need to tune the alpha (λ) parameter, which controls the penalty on coefficients.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Why Tune Hyperparameters?**  \n",
    "- To **balance bias and variance** (avoid overfitting or underfitting).  \n",
    "- To **select the best regularization strength** for better performance.  \n",
    "- To **optimize model performance** on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methods for Hyperparameter Tuning**\n",
    "1️⃣ **Grid Search (GridSearchCV)**  \n",
    "   - Tests all possible combinations of hyperparameters and selects the one with the best performance.\n",
    "   - Ensures the best combination but is **slow** for large search spaces.  \n",
    "   - Example:\n",
    "   ```python\n",
    "   from sklearn.model_selection import GridSearchCV\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "   model = RandomForestClassifier()\n",
    "   param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15]}\n",
    "\n",
    "   grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "   grid_search.fit(X_train, y_train)\n",
    "\n",
    "   print(\"Best Parameters:\", grid_search.best_params_)\n",
    "   print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "2️⃣ **Random Search (RandomizedSearchCV)**  \n",
    "   - Selects random hyperparameter combinations.  \n",
    "   - Faster than Grid Search but may miss the optimal combination.  \n",
    "   - Example:\n",
    "   ```python\n",
    "   from sklearn.model_selection import RandomizedSearchCV\n",
    "   import numpy as np\n",
    "\n",
    "   param_dist = {'n_estimators': np.arange(50, 500, 50), 'max_depth': np.arange(5, 50, 5)}\n",
    "\n",
    "   random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "   random_search.fit(X_train, y_train)\n",
    "\n",
    "   print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "✔ **GridSearchCV** is best when we have a **small number of hyperparameters**.  \n",
    "✔ **RandomizedSearchCV** is faster for **large search spaces**.  \n",
    "✔ **Lasso (L1) regression** can be tuned the same way as Ridge by replacing `Ridge()` with `Lasso()`.  \n",
    "✔ **Cross-validation ensures we get a robust hyperparameter selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36758269-f29b-4790-af16-d9efcc892cd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1d1f3-9a55-4545-8107-b7d60431f481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
